# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_tensor.ipynb.

# %% auto 0
__all__ = ['Tensor', 'AddTensor', 'SubTensor', 'MulTensor', 'MatMulTensor', 'SumTensor', 'SigmoidTensor']

# %% ../nbs/00_tensor.ipynb 5
class Tensor:
    op = "L"
    name: str = ""
    parents: list = []

    def __init__(self, data, name=""):
        self.data = data
        self.name = name
        self.grad = np.zeros_like(self.data)

    # XXXX Fix broadcasting!
    def broadcast(self, other_shape):
        if self.data.shape == other_shape:
            return self

    def add(self, other, name):
        out = AddTensor(data=self.data + other.data, name=name)
        out.parents = [self, other]
        return out

    def sub(self, other, name):
        out = SubTensor(data=self.data - other.data, name=name)
        out.parents = [self, other]
        return out

    def mul(self, other, name):
        out = MulTensor(data=self.data * other.data, name=name)
        out.parents = [self, other]
        return out

    def mmul(self, other, name):
        out = MatMulTensor(data=np.matmul(self.data, other.data), name=name)
        out.parents = [self, other]
        return out

    def sum(self, name):
        out = SumTensor(data=self.data.sum(), name=name)
        out.parents = [self]
        return out

    def sigmoid(self, name):
        out = SigmoidTensor(data=1 / (1 + np.exp(-self.data)), name=name)
        out.parents = [self]
        return out

    def backward(self):
        # Create a list of all parent nodes, in reverse order
        # Start with the current node
        visited = []
        nodes = []

        def walk(node):
            for p in node.parents:
                if p not in visited:
                    visited.append(p)
                    walk(p)
                    nodes.append(p)

        walk(self)
        nodes.append(self)

        # print(nodes)
        self.grad = np.ones_like(self.data)
        for n in nodes[::-1]:
            if hasattr(n, "_backward"):
                n._backward()

    def __repr__(self):
        res = f"[{self.op or ''}] {self.name}={str(self.data)} {self.name}.grad={str(self.grad)}"
        if self.parents:
            res += (
                f" {self.name}.parents=["
                + ",".join([p.name for p in self.parents])
                + "]"
            )

        return f"Tensor({res})"


class AddTensor(Tensor):
    op = "+"

    def _backward(self):
        self.parents[0].grad += self.grad
        self.parents[1].grad += self.grad


class SubTensor(Tensor):
    op = "-"

    def _backward(self):
        self.parents[0].grad += self.grad
        self.parents[1].grad -= self.grad


class MulTensor(Tensor):
    op = "*"

    def _backward(self):
        self.parents[0].grad += self.grad * self.parents[1].data
        self.parents[1].grad += self.grad * self.parents[0].data


class MatMulTensor(Tensor):
    op = "@"

    def _backward(self):
        # print(self.grad)
        self.parents[0].grad += np.matmul(self.grad, self.parents[1].data.T)
        self.parents[1].grad += np.matmul(self.parents[0].data.T, self.grad)


class SumTensor(Tensor):
    op = "sum"

    def _backward(self):
        self.parents[0].grad += self.grad


class SigmoidTensor(Tensor):
    op = "sigmoid"

    def _backward(self):
        self.parents[0].grad += self.grad * self.data * (1 - self.data)
