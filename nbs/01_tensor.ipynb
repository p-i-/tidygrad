{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def calculate_target_shape(s1, s2):\n",
    "    \"\"\"Calculate the target shape for broadcasting two tensors\"\"\"\n",
    "\n",
    "    # expand shaped to be the same length. Note (1,) * <negative> is empty\n",
    "    s2 = (1,) * (len(s1) - len(s2)) + s2\n",
    "    s1 = (1,) * (len(s2) - len(s1)) + s1\n",
    "\n",
    "    out_shape = ()\n",
    "    for dims in list(zip(reversed(s1), reversed(s2))):\n",
    "        if dims[0] != 1 and dims[1] != 1 and dims[0] != dims[1]:\n",
    "            raise ValueError(f\"Cannot broadcast {s1} and {s2}\")\n",
    "        out_shape = (max(dims),) + out_shape\n",
    "\n",
    "    return out_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def maybe_broadcast_elementwise(a: Tensor, b: Tensor):\n",
    "    \"\"\"Broadcast two tensors if they have different shapes\"\"\"\n",
    "    if a.data.shape != b.data.shape:\n",
    "        target_shape = calculate_target_shape(a.data.shape, b.data.shape)\n",
    "        # print(\n",
    "        #     f\"Elementwise broadcasted {a.data.shape} and {b.data.shape} to {target_shape}\"\n",
    "        # )\n",
    "        a = a.broadcast(target_shape) if a.data.shape != target_shape else a\n",
    "        b = b.broadcast(target_shape) if b.data.shape != target_shape else b\n",
    "\n",
    "    return a, b\n",
    "\n",
    "\n",
    "def maybe_broadcast_matmul(a: Tensor, b: Tensor):\n",
    "    \"\"\"Broadcast two tensors if they have different shapes, except for the last two dimensions\"\"\"\n",
    "\n",
    "    a_short_shape = a.data.shape[:-2]\n",
    "    b_short_shape = b.data.shape[:-2]\n",
    "\n",
    "    if a_short_shape != b_short_shape:\n",
    "        target_shape = calculate_target_shape(a_short_shape, b_short_shape)\n",
    "        # print(\n",
    "        #     f\"Matmul broadcasted {a.data.shape} and {b.data.shape} to {target_shape + a.data.shape[-2:]} and {target_shape + b.data.shape[-2:]}\"\n",
    "        # )\n",
    "        a = (\n",
    "            a.broadcast(target_shape + a.data.shape[-2:])\n",
    "            if a_short_shape != target_shape\n",
    "            else a\n",
    "        )\n",
    "        b = (\n",
    "            b.broadcast(target_shape + b.data.shape[-2:])\n",
    "            if b_short_shape != target_shape\n",
    "            else b\n",
    "        )\n",
    "\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class BaseOp:\n",
    "    \"\"\"Base class for all operations\"\"\"\n",
    "\n",
    "    name_template = \"??\"\n",
    "\n",
    "    def __init__(self, *args, name: str = None):\n",
    "        assert isinstance(\n",
    "            name, (str, type(None))\n",
    "        ), f\"name= should be str, got {type(name)}. You probably meant something else.\"\n",
    "\n",
    "        self.args = [\n",
    "            arg if isinstance(arg, Tensor) else Tensor(data=np.asarray(arg, dtype=np.float32))\n",
    "            for arg in args\n",
    "        ]\n",
    "        self.name = (\n",
    "            self.name_template.format(*[arg.name for arg in self.args])\n",
    "            if name is None\n",
    "            else name\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({', '.join([str(arg) for arg in self.args])})\"\n",
    "\n",
    "\n",
    "class BinaryElementwiseOp(BaseOp):\n",
    "    \"\"\"Base class for binary elementwise operations\"\"\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b)\n",
    "        self.parents = self.args = maybe_broadcast_elementwise(*self.args)\n",
    "\n",
    "\n",
    "class UnaryElementwiseOp(BaseOp):\n",
    "    \"\"\"Base class for unary elementwise operations\"\"\"\n",
    "\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a)\n",
    "        self.parents = self.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "class Load(BaseOp):\n",
    "    \"\"\"Load a tensor\"\"\"\n",
    "\n",
    "    name_template = \"?\"\n",
    "\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.parents = []\n",
    "\n",
    "\n",
    "class Add(BinaryElementwiseOp):\n",
    "    \"\"\"Add two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}+{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        self.out = Tensor(\n",
    "            data=self.args[0].data + self.args[1].data, name=self.name, op=self\n",
    "        )\n",
    "\n",
    "    def backward(self):\n",
    "        self.parents[0].grad += self.out.grad\n",
    "        self.parents[1].grad += self.out.grad\n",
    "\n",
    "\n",
    "class Sub(BinaryElementwiseOp):\n",
    "    \"\"\"Subtract two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}-{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        self.out = Tensor(\n",
    "            data=self.args[0].data - self.args[1].data, name=self.name, op=self\n",
    "        )\n",
    "\n",
    "    def backward(self):\n",
    "        self.parents[0].grad += self.out.grad\n",
    "        self.parents[1].grad -= self.out.grad\n",
    "\n",
    "\n",
    "class Mul(BinaryElementwiseOp):\n",
    "    \"\"\"Multiply two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}*{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        self.out = Tensor(\n",
    "            data=self.args[0].data * self.args[1].data, name=self.name, op=self\n",
    "        )\n",
    "\n",
    "    def backward(self):\n",
    "        self.parents[0].grad += self.out.grad * self.parents[1].data\n",
    "        self.parents[1].grad += self.out.grad * self.parents[0].data\n",
    "\n",
    "\n",
    "class Neg(UnaryElementwiseOp):\n",
    "    \"\"\"Negate a tensor\"\"\"\n",
    "\n",
    "    name_template = \"(-{}0\"\n",
    "\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        self.out = Tensor(-self.args[0].data, name=self.name, op=self)\n",
    "\n",
    "    def backward(self):\n",
    "        self.parents[0].grad -= self.out.grad\n",
    "\n",
    "\n",
    "class Log(UnaryElementwiseOp):\n",
    "    \"\"\"Take the natural logarithm of a tensor\"\"\"\n",
    "\n",
    "    name_template = \"log({})\"\n",
    "\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "\n",
    "        self.out = Tensor(np.log(self.args[0].data), name=self.name, op=self)\n",
    "\n",
    "    def backward(self):\n",
    "        self.parents[0].grad += self.out.grad / self.parents[0].data\n",
    "\n",
    "\n",
    "class Matmul(BaseOp):\n",
    "    \"\"\"Matrix multiplication of two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}@{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        self.parents = self.args = maybe_broadcast_matmul(*self.args)\n",
    "        self.out = Tensor(\n",
    "            np.matmul(self.args[0].data, self.args[1].data),\n",
    "            name=self.name,\n",
    "            op=self,\n",
    "        )\n",
    "\n",
    "    def backward(self):\n",
    "        self.parents[0].grad += np.matmul(\n",
    "            self.out.grad, self.parents[1].data.swapaxes(-1, -2)\n",
    "        )\n",
    "        self.parents[1].grad += np.matmul(\n",
    "            self.parents[0].data.swapaxes(-1, -2), self.out.grad\n",
    "        )\n",
    "\n",
    "\n",
    "class Sum(BaseOp):\n",
    "    \"\"\"Sum a tensor along the given axis (int or tuple of ints)\"\"\"\n",
    "\n",
    "    name_template = \"sum({})\"\n",
    "\n",
    "    def __init__(self, a, name=None, axis=None):\n",
    "        super().__init__(a, name=name)\n",
    "        # self.axis = axis\n",
    "        self.parents = self.args\n",
    "        self.out = Tensor(np.sum(self.args[0].data, axis=axis), name=self.name, op=self)\n",
    "\n",
    "    def backward(self):\n",
    "        self.parents[0].grad += self.out.grad \n",
    "\n",
    "\n",
    "class Broadcast(BaseOp):\n",
    "    \"\"\"Broadcast a tensor to the given shape\"\"\"\n",
    "\n",
    "    name_template = \"broadcast({})\"\n",
    "\n",
    "    def __init__(self, a, target_shape, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        self.target_shape = target_shape\n",
    "        self.parents = self.args\n",
    "        self_shape = self.args[0].data.shape\n",
    "        assert self_shape != target_shape, \"Why are you broadcasting to the same shape?\"\n",
    "\n",
    "        if len(self_shape) < len(target_shape):\n",
    "            expanded_shape = (len(target_shape) - len(self_shape)) * (1,) + self_shape\n",
    "        else:\n",
    "            expanded_shape = self_shape\n",
    "\n",
    "        final_shape = ()\n",
    "        broadcasted_dims = ()\n",
    "\n",
    "        for s_expanded, s_target in reversed(list(zip(expanded_shape, target_shape))):\n",
    "            if s_expanded != s_target:\n",
    "                if s_expanded != 1:\n",
    "                    raise ValueError(f\"Cannot broadcast {self_shape} to {target_shape}\")\n",
    "                else:\n",
    "                    broadcasted_dims = (True,) + broadcasted_dims\n",
    "                    final_shape = (s_target,) + final_shape\n",
    "            else:\n",
    "                broadcasted_dims = (False,) + broadcasted_dims\n",
    "                final_shape = (s_expanded,) + final_shape\n",
    "\n",
    "        broadcasted_data = np.broadcast_to(self.args[0].data, final_shape)\n",
    "\n",
    "        assert final_shape == broadcasted_data.shape\n",
    "\n",
    "        data = broadcasted_data\n",
    "        self.broadcasted_dims = broadcasted_dims\n",
    "\n",
    "        self.out = Tensor(data, name=self.name, op=self)\n",
    "\n",
    "    def backward(self):\n",
    "        axis = tuple([i for i, dim in enumerate(self.broadcasted_dims) if dim])\n",
    "        summed = self.out.grad.sum(axis=axis, keepdims=True)\n",
    "\n",
    "        if summed.shape != self.parents[0].data.shape:\n",
    "            summed = summed.reshape(self.parents[0].data.shape)\n",
    "\n",
    "        self.parents[0].grad += summed\n",
    "\n",
    "\n",
    "# class LessThan(BinaryElementwiseOp):\n",
    "#     name_template = \"({}<{})\"\n",
    "\n",
    "#     def __init__(self, a, b, name=None):\n",
    "#         super().__init__(a, b, name=name)\n",
    "#         self.out = Tensor(\n",
    "#             data=self.args[0].data < self.args[1].data, name=self.name, op=self\n",
    "#         )\n",
    "\n",
    "#     # def backward(self):\n",
    "#     #     self.parents[0].grad += self.out.grad * (self.parents[0].data < self.parents[1].data)\n",
    "#     #     self.parents[1].grad += self.out.grad * (self.parents[0].data >= self.parents[1].data)\n",
    "\n",
    "\n",
    "# class Where(BaseOp):\n",
    "#     name_template = \"where({})\"\n",
    "\n",
    "#     def __init__(self, a, b, c, name=None):\n",
    "#         super().__init__(a, b, c, name=name)\n",
    "#         self.parents = self.args\n",
    "#         self.out = Tensor(\n",
    "#             data=np.where(self.args[0].data, self.args[1].data, self.args[2].data),\n",
    "#             name=self.name,\n",
    "#             op=self,\n",
    "#         )\n",
    "\n",
    "#     def backward(self):\n",
    "#         # self.parents[0].grad += self.out.grad * self.parents[1].data\n",
    "#         # self.parents[0].grad += self.out.grad * self.parents[2].data\n",
    "\n",
    "#         self.parents[1].grad += self.out.grad * self.parents[0].data\n",
    "#         self.parents[2].grad += self.out.grad * (1 - self.parents[0].data)\n",
    "\n",
    "\n",
    "class ExpLog(UnaryElementwiseOp):\n",
    "    \"\"\"Exponentiate a tensor\"\"\"\n",
    "\n",
    "    name_template = \"exp({})\"\n",
    "\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "\n",
    "        def logexp(x):\n",
    "            return np.where(x < 0, np.log(1 + np.exp(x)), x + np.log(1 + np.exp(-x)))\n",
    "\n",
    "        self.out = Tensor(logexp(self.args[0].data), name=self.name, op=self)\n",
    "\n",
    "    def backward(self):\n",
    "        self.parents[0].grad += self.out.grad * (\n",
    "            1 - 1 / (1 + np.exp(self.parents[0].data))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    # op = \"L\"\n",
    "    name: str = \"\"\n",
    "\n",
    "    def __init__(self, data, name=None, op=None, eps=1e-8):\n",
    "        self.data = np.asarray(data)\n",
    "        self.grad = np.zeros_like(self.data, dtype=np.float32)\n",
    "        self.eps = eps\n",
    "        self.op = op or Load(name=name)\n",
    "        self.name = name or self.op.name\n",
    "\n",
    "    def __repr__(self):\n",
    "        if repr(self.data):\n",
    "            value_str = f\"\\n    v={np.array2string(self.data, prefix='     ')})\"\n",
    "            grad_str = f\"\\n    ∇={np.array2string(self.grad, prefix='     ')}\"\n",
    "        else:\n",
    "            value_str = f\" v={str(self.data)}\"\n",
    "            grad_str = f\" ∇={str(self.grad)}\"\n",
    "\n",
    "        parents = (\n",
    "            f\" {self.name}.parents=[\" + \",\".join([p.name for p in self.op.parents]) + \"]\"\n",
    "            if self.op.parents\n",
    "            else \"\"\n",
    "        )\n",
    "\n",
    "        return f'Tensor{list(self.data.shape)}(name=\"{self.name}\" op={type(self.op).__name__}{parents}{value_str}{grad_str})'\n",
    "\n",
    "    def broadcast(self, target_shape, name=None):\n",
    "        return Broadcast(self, target_shape, name=name).out\n",
    "\n",
    "    def add(self, other, name=None):\n",
    "        return Add(self, other, name=name).out\n",
    "\n",
    "    def sub(self, other, name=None):\n",
    "        return Sub(self, other, name=name).out\n",
    "\n",
    "    def mul(self, other, name=None):\n",
    "        return Mul(self, other, name=name).out\n",
    "\n",
    "    def neg(self, name=None):\n",
    "        return Neg(self, name=name).out\n",
    "\n",
    "    def log(self, name=None):\n",
    "        return Log(self, name=name).out\n",
    "\n",
    "    def mmul(self, other, name=None):\n",
    "        return Matmul(self, other, name=name).out\n",
    "\n",
    "    def sum(self, name=None, axis=None):\n",
    "        return Sum(self, name=name, axis=axis).out\n",
    "\n",
    "    # def lt(self, other, name=None):\n",
    "    #     return LessThan(self, other, name=name).out\n",
    "\n",
    "    # def where(self, other1, other2, name=None):\n",
    "    #     return Where(self, other1, other2, name=name).out\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return self.add(other)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self.sub(other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return self.mul(other)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self.neg()\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    # def __lt__(self, other):\n",
    "    #     return self.lt(other)\n",
    "\n",
    "    def backward(self):\n",
    "        # Create a list of all parent nodes, in reverse order\n",
    "        # Start with the current node\n",
    "        visited = []\n",
    "        nodes = []\n",
    "\n",
    "        assert self.data.size == 1, \"Cannot call backward on non-scalar tensor\"\n",
    "\n",
    "        def walk(node):\n",
    "            for p in node.op.parents:\n",
    "                if p not in visited:\n",
    "                    visited.append(p)\n",
    "                    walk(p)\n",
    "                    nodes.append(p)\n",
    "\n",
    "        walk(self)\n",
    "        nodes.append(self)\n",
    "\n",
    "        # print(nodes)\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        for n in nodes[::-1]:\n",
    "            if hasattr(n.op, \"backward\"):\n",
    "                n.op.backward()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_eq, test_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(calculate_target_shape((1, 2, 3), (2, 3)), (1, 2, 3))\n",
    "test_eq(calculate_target_shape((1, 2, 3), (2, 1)), (1, 2, 3))\n",
    "test_eq(calculate_target_shape((1, 2, 3), (1, 3)), (1, 2, 3))\n",
    "test_eq(calculate_target_shape((1, 2, 3), (1, 1)), (1, 2, 3))\n",
    "\n",
    "test_eq(calculate_target_shape((1, 5), (3, 1)), (3, 5))\n",
    "\n",
    "test_fail(calculate_target_shape, args=((1, 2, 3), (2, 2)), contains=\"Cannot broadcast\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
