{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lovely_numpy import Lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array[1, 1, 2, 3] n=6 x∈[0.543, 0.948] μ=0.729 σ=0.155 [[[[0.558, 0.681, 0.948], [0.543, 0.744, 0.899]]]]\n",
      "array[2, 1, 1, 3] n=6 x∈[0.006, 0.950] μ=0.564 σ=0.291 [[[[0.006, 0.438, 0.623]]], [[[0.950, 0.673, 0.691]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array[2, 1, 2, 3] n=12 x∈[0.549, 1.640] μ=1.292 σ=0.362"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(1, 1, 2, 3)\n",
    "b = np.random.rand(2, 1, 1, 3)\n",
    "\n",
    "\n",
    "print(Lo(a))\n",
    "print(Lo(b))\n",
    "\n",
    "\n",
    "Lo(b + a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_target_shape(s1, s2):\n",
    "    \"\"\"Calculate the target shape for broadcasting two tensors\"\"\"\n",
    "\n",
    "    # expand shaped to be the same length. Note (1,) * <negative> is empty\n",
    "    s2 = (1,) * (len(s1) - len(s2)) + s2\n",
    "    s1 = (1,) * (len(s2) - len(s1)) + s1\n",
    "\n",
    "    out_shape = ()\n",
    "    for dims in list(zip(reversed(s1), reversed(s2))):\n",
    "        if dims[0] != 1 and dims[1] != 1 and dims[0] != dims[1]:\n",
    "            raise ValueError(f\"Cannot broadcast {s1} and {s2}\")\n",
    "        out_shape = (max(dims),) + out_shape\n",
    "\n",
    "    return out_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_broadcast_elementwise(a, b):\n",
    "    \"\"\"Broadcast two tensors if they have different shapes\"\"\"\n",
    "    if a.data.shape != b.data.shape:\n",
    "        target_shape = calculate_target_shape(a.data.shape, b.data.shape)\n",
    "        a = a.broadcast(target_shape)\n",
    "        b = b.broadcast(target_shape)\n",
    "\n",
    "        print(\n",
    "            f\"Elementwise broadcasted {a.data.shape} and {b.data.shape} to {a.data.shape} and {b.data.shape}\"\n",
    "        )\n",
    "\n",
    "    return a, b\n",
    "\n",
    "\n",
    "def maybe_broadcast_matmul(a, b):\n",
    "    \"\"\"Broadcast two tensors if they have different shapes, except for the last two dimensions\"\"\"\n",
    "\n",
    "    a_short_shape = a.data.shape[:-2]\n",
    "    b_short_shape = b.data.shape[:-2]\n",
    "\n",
    "    if a_short_shape != b_short_shape:\n",
    "        target_shape = calculate_target_shape(a_short_shape, b_short_shape)\n",
    "        a = a.broadcast(target_shape + a.data.shape[-2:])\n",
    "        b = b.broadcast(target_shape + b.data.shape[-2:])\n",
    "\n",
    "        print(\n",
    "            f\"Matmul broadcasted {a.data.shape} and {b.data.shape} to {a.data.shape} and {b.data.shape}\"\n",
    "        )\n",
    "\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    op = \"L\"\n",
    "    name: str = \"\"\n",
    "    parents: list = []\n",
    "\n",
    "    def __init__(self, data, name=\"\"):\n",
    "        self.data = data\n",
    "        self.name = name\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "\n",
    "    def broadcast(self, target_shape):\n",
    "        self_shape = self.data.shape\n",
    "        if self_shape == target_shape:\n",
    "            return self\n",
    "\n",
    "        if len(self_shape) < len(target_shape):\n",
    "            expanded_shape = (len(target_shape) - len(self_shape)) * (1,) + self_shape\n",
    "        else:\n",
    "            expanded_shape = self_shape\n",
    "\n",
    "        final_shape = ()\n",
    "        broadcasted_dims = ()\n",
    "\n",
    "        for s_expanded, s_target in reversed(list(zip(expanded_shape, target_shape))):\n",
    "            if s_expanded != s_target:\n",
    "                if s_expanded != 1:\n",
    "                    raise ValueError(f\"Cannot broadcast {self_shape} to {target_shape}\")\n",
    "                else:\n",
    "                    broadcasted_dims = (True,) + broadcasted_dims\n",
    "                    final_shape = (s_target,) + final_shape\n",
    "\n",
    "            else:\n",
    "                broadcasted_dims = (False,) + broadcasted_dims\n",
    "                final_shape = (s_expanded,) + final_shape\n",
    "\n",
    "        broadcasted_data = np.broadcast_to(self.data, final_shape)\n",
    "\n",
    "        assert final_shape == broadcasted_data.shape\n",
    "\n",
    "        out = BroadcastTensor(data=broadcasted_data, name=self.name + \"_broadcasted\")\n",
    "        out.parents = [self]\n",
    "        out.broadcasted_dims = broadcasted_dims\n",
    "        return out\n",
    "\n",
    "    def add(self, other, name):\n",
    "        a, b = maybe_broadcast_elementwise(self, other)\n",
    "\n",
    "        out = AddTensor(data=a.data + b.data, name=name)\n",
    "        out.parents = [a, b]\n",
    "        return out\n",
    "\n",
    "    def sub(self, other, name):\n",
    "        a, b = maybe_broadcast_elementwise(self, other)\n",
    "        out = SubTensor(data=a.data - b.data, name=name)\n",
    "        out.parents = [a, b]\n",
    "        return out\n",
    "\n",
    "    def mul(self, other, name):\n",
    "        a, b = maybe_broadcast_elementwise(self, other)\n",
    "        out = MulTensor(data=a.data * b.data, name=name)\n",
    "        out.parents = [a, b]\n",
    "        return out\n",
    "\n",
    "    def mmul(self, other, name):\n",
    "        a, b = maybe_broadcast_matmul(self, other)\n",
    "        out = MatMulTensor(data=np.matmul(a.data, b.data), name=name)\n",
    "        out.parents = [a, b]\n",
    "        return out\n",
    "\n",
    "    def sum(self, name):\n",
    "        out = SumTensor(data=self.data.sum(), name=name)\n",
    "        out.parents = [self]\n",
    "        return out\n",
    "\n",
    "    def sigmoid(self, name):\n",
    "        out = SigmoidTensor(data=1 / (1 + np.exp(-self.data)), name=name)\n",
    "        out.parents = [self]\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        # Create a list of all parent nodes, in reverse order\n",
    "        # Start with the current node\n",
    "        visited = []\n",
    "        nodes = []\n",
    "\n",
    "        def walk(node):\n",
    "            for p in node.parents:\n",
    "                if p not in visited:\n",
    "                    visited.append(p)\n",
    "                    walk(p)\n",
    "                    nodes.append(p)\n",
    "\n",
    "        walk(self)\n",
    "        nodes.append(self)\n",
    "\n",
    "        # print(nodes)\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        for n in nodes[::-1]:\n",
    "            if hasattr(n, \"_backward\"):\n",
    "                n._backward()\n",
    "\n",
    "    def __repr__(self):\n",
    "        value_repr = repr(self.data)\n",
    "        if \"\\n\" in value_repr:\n",
    "            res = f\"\\n{self.name}=\\n{str(self.data)}\\n{self.name}.grad=\\n{str(self.grad)}\"\n",
    "        else:\n",
    "            res = f\"{self.name}={str(self.data)} {self.name}.grad={str(self.grad)}\"\n",
    "\n",
    "        if self.parents:\n",
    "            res += (\n",
    "                f\" {self.name}.parents=[\" + \",\".join([p.name for p in self.parents]) + \"]\"\n",
    "            )\n",
    "\n",
    "        return f\"Tensor(shape=[{self.data.shape}])] op={self.op or ''} {res}\"\n",
    "\n",
    "\n",
    "class AddTensor(Tensor):\n",
    "    op = \"+\"\n",
    "\n",
    "    def _backward(self):\n",
    "        self.parents[0].grad += self.grad\n",
    "        self.parents[1].grad += self.grad\n",
    "\n",
    "\n",
    "class SubTensor(Tensor):\n",
    "    op = \"-\"\n",
    "\n",
    "    def _backward(self):\n",
    "        self.parents[0].grad += self.grad\n",
    "        self.parents[1].grad -= self.grad\n",
    "\n",
    "\n",
    "class MulTensor(Tensor):\n",
    "    op = \"*\"\n",
    "\n",
    "    def _backward(self):\n",
    "        self.parents[0].grad += self.grad * self.parents[1].data\n",
    "        self.parents[1].grad += self.grad * self.parents[0].data\n",
    "\n",
    "\n",
    "class MatMulTensor(Tensor):\n",
    "    op = \"@\"\n",
    "\n",
    "    def _backward(self):\n",
    "        # print(self.grad)\n",
    "        self.parents[0].grad += np.matmul(self.grad, self.parents[1].data.T)\n",
    "        self.parents[1].grad += np.matmul(self.parents[0].data.T, self.grad)\n",
    "\n",
    "\n",
    "class SumTensor(Tensor):\n",
    "    op = \"sum\"\n",
    "\n",
    "    def _backward(self):\n",
    "        self.parents[0].grad += self.grad\n",
    "\n",
    "\n",
    "class SigmoidTensor(Tensor):\n",
    "    op = \"sigmoid\"\n",
    "\n",
    "    def _backward(self):\n",
    "        self.parents[0].grad += self.grad * self.data * (1 - self.data)\n",
    "\n",
    "\n",
    "class BroadcastTensor(Tensor):\n",
    "    op = \"broadcast\"\n",
    "\n",
    "    def _backward(self):\n",
    "        axis = tuple([i for i, dim in enumerate(self.broadcasted_dims) if dim])\n",
    "        summed = self.grad.sum(axis=axis, keepdims=True)\n",
    "\n",
    "        if summed.shape != self.parents[0].data.shape:\n",
    "            summed = summed.reshape(self.parents[0].data.shape)\n",
    "\n",
    "        self.parents[0].grad += summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_eq, test_fail, ExceptionExpected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(calculate_target_shape((1, 2, 3), (2, 3)), (1, 2, 3))\n",
    "test_eq(calculate_target_shape((1, 2, 3), (2, 1)), (1, 2, 3))\n",
    "test_eq(calculate_target_shape((1, 2, 3), (1, 3)), (1, 2, 3))\n",
    "test_eq(calculate_target_shape((1, 2, 3), (1, 1)), (1, 2, 3))\n",
    "\n",
    "test_eq(calculate_target_shape((1, 5), (3, 1)), (3, 5))\n",
    "\n",
    "# with ExceptionExpected(ValueError):\n",
    "test_fail(calculate_target_shape, args=((1, 2, 3), (2, 2)), contains=\"Cannot broadcast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matmul broadcasted (1, 8, 5) and (1, 5, 3) to (1, 8, 5) and (1, 5, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m pred \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mmmul(weight, \u001b[39m\"\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m loss \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39msum(\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     15\u001b[0m display(data, weight, pred, loss)\n",
      "Cell \u001b[0;32mIn[51], line 104\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nodes[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m    103\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(n, \u001b[39m\"\u001b[39m\u001b[39m_backward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 104\u001b[0m         n\u001b[39m.\u001b[39;49m_backward()\n",
      "Cell \u001b[0;32mIn[51], line 151\u001b[0m, in \u001b[0;36mMatMulTensor._backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_backward\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    150\u001b[0m     \u001b[39m# print(self.grad)\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mgrad \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmatmul(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparents[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparents[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mgrad \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparents[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mT, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 3)"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.random.randn(8, 5), \"data\")\n",
    "weight = Tensor(np.random.randn(1, 5, 3), \"weight\")\n",
    "\n",
    "# data shape [B, X]\n",
    "\n",
    "# weight shape [in, out]\n",
    "\n",
    "\n",
    "pred = data.mmul(weight, \"s\")\n",
    "loss = pred.sum(\"loss\")\n",
    "\n",
    "loss.backward()\n",
    "display(data, weight, pred, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | eval: false\n",
    "np.isclose(a_rand.grad, a.grad.numpy()).all()\n",
    "np.isclose(b_rand.grad, b.grad.numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[(4, 2)])] op=L \n",
       "x=\n",
       "[[0. 0.]\n",
       " [1. 0.]\n",
       " [0. 1.]\n",
       " [1. 1.]]\n",
       "x.grad=\n",
       "[[0. 0.]\n",
       " [0. 0.]\n",
       " [0. 0.]\n",
       " [0. 0.]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[(2, 3)])] op=L \n",
       "w1=\n",
       "[[-1.15453297e-01 -1.13189763e-01 -1.05737511e-04]\n",
       " [ 7.09011434e-02 -9.62888928e-02  1.68078661e-01]]\n",
       "w1.grad=\n",
       "[[0. 0. 0.]\n",
       " [0. 0. 0.]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[(4, 1)])] op=* \n",
       "loss=\n",
       "[[0.25      ]\n",
       " [0.25391063]\n",
       " [0.25601467]\n",
       " [0.24022402]]\n",
       "loss.grad=\n",
       "[[1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]] loss.parents=[diff,diff]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[(4, 1)])] op=sigmoid \n",
       "preds=\n",
       "[[0.5       ]\n",
       " [0.49610454]\n",
       " [0.49402107]\n",
       " [0.49012653]]\n",
       "preds.grad=\n",
       "[[ 1.        ]\n",
       " [-1.00779092]\n",
       " [-1.01195785]\n",
       " [ 0.98025307]] preds.parents=[z2]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[(4, 3)])] op=@ \n",
       "z1=\n",
       "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
       " [-1.15453297e-01 -1.13189763e-01 -1.05737511e-04]\n",
       " [ 7.09011434e-02 -9.62888928e-02  1.68078661e-01]\n",
       " [-4.45521539e-02 -2.09478656e-01  1.67972924e-01]]\n",
       "z1.grad=\n",
       "[[-0.00337133  0.03786638 -0.01245886]\n",
       " [ 0.00339739 -0.03815908  0.01255517]\n",
       " [ 0.00341115 -0.0383137   0.01260604]\n",
       " [-0.00330346  0.03710416 -0.01220808]] z1.parents=[x,w1]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[(2, 3)])] op=L \n",
       "w1=\n",
       "[[-0.11547208 -0.11297878 -0.00017516]\n",
       " [ 0.07087961 -0.09604698  0.16799907]]\n",
       "w1.grad=\n",
       "[[0. 0. 0.]\n",
       " [0. 0. 0.]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[(1, 3)])] op=L b1=[[0. 0. 0.]] b1.grad=[[0. 0. 0.]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[(3, 1)])] op=L \n",
       "w2=\n",
       "[[-0.01353289]\n",
       " [ 0.15115407]\n",
       " [-0.04956716]]\n",
       "w2.grad=\n",
       "[[0.]\n",
       " [0.]\n",
       " [0.]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [\n",
    "    [0, 0],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 1],\n",
    "]\n",
    "\n",
    "Y = [0, 1, 1, 0]\n",
    "\n",
    "w1 = Tensor(np.random.randn(2, 3) * 0.1, \"w1\")\n",
    "b1 = Tensor(np.zeros((1, 3)), \"b1\")\n",
    "\n",
    "w2 = Tensor(np.random.randn(3, 1) * 0.1, \"w2\")\n",
    "\n",
    "LR = 0.2\n",
    "\n",
    "# print(w1, b1, w2)\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    # for x, y in zip(X, Y):\n",
    "    x = Tensor(np.array(X, dtype=float), \"x\")\n",
    "    y = Tensor(np.array(Y, dtype=float)[:, None], \"y\")\n",
    "\n",
    "    display(x)\n",
    "    display(w1)\n",
    "\n",
    "    z1 = x.mmul(w1, \"z1\")  # .add(b1, \"z1\").sigmoid(\"z1\")\n",
    "    preds = z1.mmul(w2, \"z2\").sigmoid(\"preds\")\n",
    "\n",
    "    # display(preds)\n",
    "\n",
    "    diff = preds.sub(y, \"diff\")\n",
    "    # display(diff)\n",
    "\n",
    "    l = diff.mul(diff, \"loss\")\n",
    "    loss = l.sum(\"loss\")\n",
    "\n",
    "    # print(loss)\n",
    "    # epoch_loss += loss.data\n",
    "\n",
    "    loss.backward()\n",
    "    display(l)\n",
    "    display(preds)\n",
    "    display(z1)\n",
    "\n",
    "    w1.data -= LR * w1.grad\n",
    "    b1.data -= LR * b1.grad\n",
    "    w2.data -= LR * w2.grad\n",
    "\n",
    "    w1.grad = np.zeros_like(w1.grad)\n",
    "    b1.grad = np.zeros_like(b1.grad)\n",
    "    w2.grad = np.zeros_like(w2.grad)\n",
    "\n",
    "    display(w1, b1, w2)\n",
    "\n",
    "    losses.append(loss.data)\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.yscale(\"log\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
