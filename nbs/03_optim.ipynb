{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp optim\n",
    "# | hide\n",
    "import nbdev\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "import numpy as np\n",
    "from tidygrad.tensor import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr=0.1, mom=0):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.mom = mom\n",
    "        self.moms = [np.zeros_like(p.data) for p in params]\n",
    "\n",
    "    def step(self):\n",
    "        for i, p in enumerate(self.params):\n",
    "            if not np.isnan(p.grad).any() or np.isinf(p.grad).any():\n",
    "                self.moms[i] = self.mom * self.moms[i] + (1 - self.mom) * p.grad\n",
    "                p.data -= self.lr * self.moms[i]  # / (1 - self.mom)\n",
    "            else:\n",
    "                print(\"NaN gradient encountered, skipping step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.ms = [np.zeros_like(p.data) for p in params]\n",
    "        self.vs = [np.zeros_like(p.data) for p in params]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, p in enumerate(self.params):\n",
    "            self.ms[i] = self.beta1 * self.ms[i] + (1 - self.beta1) * p.grad\n",
    "            self.vs[i] = self.beta2 * self.vs[i] + (1 - self.beta2) * p.grad**2\n",
    "            m_hat = self.ms[i] / (1 - self.beta1**self.t)\n",
    "            v_hat = self.vs[i] / (1 - self.beta2**self.t)\n",
    "            p.data -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
