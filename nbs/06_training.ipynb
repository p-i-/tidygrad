{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp training\n",
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from tidygrad.tensor import Tensor\n",
    "from tidygrad.utils import noop\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def add_callbacks(func):\n",
    "    # print(\"Adding callbacks\", func.__name__)\n",
    "\n",
    "    def decorator(self):\n",
    "        full_name = func.__name__.replace(\"do_\", \"\")\n",
    "        pre_name = f\"pre_{full_name}\"\n",
    "        post_name = f\"post_{full_name}\"\n",
    "        for callback in self.callbacks:\n",
    "            getattr(callback, pre_name, noop)(self)\n",
    "\n",
    "        func(self)\n",
    "        for callback in self.callbacks:\n",
    "            getattr(callback, post_name, noop)(self)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    def __init__(self, train=True, valid=True):\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "    @staticmethod\n",
    "    def calc() -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MultiClassAccuracy(Metric):\n",
    "    name = \"accuracy\"\n",
    "\n",
    "    @staticmethod\n",
    "    def calc(learner) -> float:\n",
    "        _, y = learner.batch\n",
    "        return float((learner.preds.data.argmax(axis=-1) == y.data).mean())\n",
    "\n",
    "class Loss(Metric):\n",
    "    name = \"loss\"\n",
    "\n",
    "    @staticmethod\n",
    "    def calc(learner) -> float:\n",
    "        return float(learner.loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class DictLoggerCallback:\n",
    "    val_loss = 0\n",
    "    val_error = 0\n",
    "\n",
    "    def __init__(self, metrics=None, history=None):\n",
    "        self.metrics = [MultiClassAccuracy(), Loss()] if metrics is None else metrics\n",
    "        self.history = [] if history is None else history\n",
    "\n",
    "    def log(self, learner, metric, value, accum=False, step: int = None):\n",
    "        if not hasattr(learner, \"history\"): learner.history = self.history\n",
    "        if step is None:\n",
    "            step = self.history[-1][\"step\"] + 1 if self.history else 0\n",
    "\n",
    "        if not self.history or step != self.history[-1][\"step\"]:\n",
    "            self.history.append({\"step\": step})\n",
    "\n",
    "        if metric in self.history[-1] and accum:\n",
    "            self.history[-1][metric] += value\n",
    "        else:\n",
    "            self.history[-1][metric] = value\n",
    "\n",
    "    def post_calc_loss(self, learner):\n",
    "        for m in self.metrics:\n",
    "            if learner.training:\n",
    "                if m.train: self.log(learner, f\"{m.name}\", m.calc(learner), accum=False, step=learner.step)\n",
    "            else:\n",
    "                if m.valid: self.log(learner, f\"val_{m.name}\", m.calc(learner), accum=True, step=learner.step)\n",
    "\n",
    "    def post_epoch(self, learner):\n",
    "        for m in self.metrics:\n",
    "            if f\"val_{m.name}\" in self.history[-1]:\n",
    "                self.history[-1][f\"val_{m.name}\"] /= len(learner.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class Learner:\n",
    "    # dataloaders - train, test\n",
    "    # model - function that outputs a tensor that can be fed into a loss function\n",
    "    # loss_func - function that takes in a tensor and outputs a scalar\n",
    "    # optimizer - Optimizer object\n",
    "    def __init__(self, dataloaders, model, loss_func, optimizer, callbacks=[]):\n",
    "        self.dataloaders = dataloaders\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optimizer\n",
    "        self.callbacks = callbacks if callbacks else []\n",
    "\n",
    "        # The state of the learner. These are updated during training by various do_ functions\n",
    "        self.training = False  # True if training, False if val/test.\n",
    "        self.epoch = 0  # current epoch, starts with 1 when you start trainig.\n",
    "        self.step = 0  # current step, increases by 1 every (training) batch\n",
    "        self.dl = None  # current dataloader, could be train or test or val\n",
    "        self.batch = None  # The current batch as a tuple of (x, y)\n",
    "        self.preds: Tensor = None  # Output of the model\n",
    "\n",
    "    def fit(self, epochs, start_epoch=0, start_step=None):\n",
    "        self.start_epoch = start_epoch\n",
    "        self.n_epochs = epochs\n",
    "        self.step = self.step if start_step is None else start_step\n",
    "        self.epochs = range(self.start_epoch, self.start_epoch + self.n_epochs)\n",
    "        self.do_fit()\n",
    "\n",
    "    @add_callbacks\n",
    "    def do_fit(self):\n",
    "        for e in self.epochs:\n",
    "            self.epoch = e\n",
    "            self.do_epoch()\n",
    "\n",
    "    @add_callbacks\n",
    "    def do_epoch(self):\n",
    "        self.training = True\n",
    "        self.dl = self.dataloaders.train\n",
    "        self.do_all_batches()\n",
    "        self.dl = self.dataloaders.test\n",
    "        self.training = False\n",
    "        self.do_all_batches()\n",
    "\n",
    "    @add_callbacks\n",
    "    def do_all_batches(self):\n",
    "        for batch in self.dl:\n",
    "            if self.training: self.step += 1\n",
    "            self.batch = batch\n",
    "            self.do_batch_forward()\n",
    "            self.do_calc_loss()\n",
    "            if self.training: self.do_batch_backward()\n",
    "\n",
    "    @add_callbacks\n",
    "    def do_calc_loss(self):\n",
    "        _, y = self.batch\n",
    "        self.loss = self.loss_func(self.preds, y)\n",
    "\n",
    "    @add_callbacks\n",
    "    def do_batch_forward(self):\n",
    "        x, _ = self.batch\n",
    "        self.preds = self.model(x)\n",
    "\n",
    "    @add_callbacks\n",
    "    def do_batch_backward(self):\n",
    "        self.loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def one_hot_encode_batch(y, n_classes):\n",
    "    batch_size = len(y)\n",
    "    assert batch_size > 0\n",
    "    assert n_classes > 0\n",
    "    assert y.shape == (batch_size, )\n",
    "    assert np.min(y) >= 0\n",
    "\n",
    "    # Initialize a zero matrix of shape (batch_size, num_classes)\n",
    "    one_hot_matrix = np.zeros((batch_size, n_classes))\n",
    "\n",
    "    # Fill in the appropriate elements\n",
    "    one_hot_matrix[np.arange(batch_size), y] = 1\n",
    "\n",
    "    return Tensor(one_hot_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidygrad.utils.data import DataLoader, DataLoaders\n",
    "from tidygrad.utils.datasets import MNIST, mnist_batch_tfm\n",
    "from tidygrad.functional import sigmoid, BCE_loss, CrossEntropy_loss\n",
    "from tidygrad.optim import Adam\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def metrics_names_pretty(metrics):\n",
    "    \"\"\"\n",
    "    Return an str with the names of the metrics, extended to at least 8 characters\n",
    "    \"\"\"\n",
    "\n",
    "    pretty = [ f\"{name:<10}\" for name in metrics]\n",
    "    return \" \".join(pretty)\n",
    "\n",
    "def metrics_last_pretty(metrics, metrics_dict):\n",
    "    \"\"\"\n",
    "    Return an str with the last values of the metrics,\n",
    "    extended to match the length of the metric names, min 10 characters\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    for name in metrics:\n",
    "        max_len = max(len(name), 10)\n",
    "        if name in metrics_dict:\n",
    "            value = metrics_dict[name]\n",
    "\n",
    "            if isinstance(value, (int, str)):\n",
    "                value = str(value)\n",
    "            elif isinstance(value, float):\n",
    "                value = (f\"{value:.6f}\")\n",
    "            else:\n",
    "                value = \" \" * (max_len)\n",
    "\n",
    "            if len(value) > max_len:  value = value[:max_len-2] + \"..\"\n",
    "\n",
    "            out.append( value + \" \" * (max_len - len(value)))\n",
    "        else:\n",
    "            out.append(\" \" * max_len)\n",
    "\n",
    "    return \" \".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def print_metrics_header(metrics, mbar):\n",
    "    mbar.write(\"Ep  | \" + metrics_names_pretty(metrics) + \" | \" + metrics_names_pretty([f\"val_{m}\" for m in metrics]))\n",
    "\n",
    "def print_metrics(learner, metrics, mbar):\n",
    "    train_metrics ={ k: v for k, v in learner.history[-1].items() if k in metrics }\n",
    "    val_metrics = { f\"val_{m}\": learner.history[-1].get(f\"val_{m}\", None) for m in metrics }\n",
    "    mbar.write(f\"{learner.epoch:<3} | \" + metrics_last_pretty(metrics, train_metrics) + \" | \" + metrics_last_pretty([f\"val_{m}\" for m in metrics],  val_metrics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProgressBarCallback:\n",
    "    def __init__(self, metrics=[\"loss\"], plot=True):\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def pre_fit(self, learner):\n",
    "        self.mbar = tqdm(initial=learner.start_epoch, total=learner.start_epoch+learner.n_epochs, desc=\"Epochs\")\n",
    "        self.pbar = tqdm(leave=False)\n",
    "        print_metrics_header(self.metrics, self.mbar)\n",
    "\n",
    "\n",
    "    def post_epoch(self, learner):\n",
    "        self.mbar.update(1)\n",
    "        print_metrics(learner, self.metrics, self.mbar)\n",
    "        self.mbar.set_description_str(f\"Epoch\")\n",
    "\n",
    "\n",
    "    def pre_all_batches(self, learner):\n",
    "        self.pbar.reset(total=len(learner.dl))\n",
    "        if learner.training:\n",
    "            self.pbar.set_description_str(f\"{learner.epoch:<3}: Train\")\n",
    "        else:\n",
    "            self.pbar.set_description_str(f\"{learner.epoch:<3}: Valid\")\n",
    "\n",
    "    def post_calc_loss(self, learner):\n",
    "        self.pbar.update(1)\n",
    "        hist = learner.history[-1]\n",
    "        metrics = [f\"{k}={v:.4f}\" for k, v in hist.items() if k in self.metrics]\n",
    "        if not learner.training:\n",
    "            val_metrics = { f\"val_{m}\": hist[f\"val_{m}\"] for m in self.metrics if f\"val_{m}\" in hist }\n",
    "            metrics += [f\"{k}={v:.4f}\" for k, v in reversed(val_metrics.items())]\n",
    "\n",
    "        self.pbar.set_postfix_str(\" \".join(metrics), refresh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 32\n",
    "\n",
    "mnist_train = DataLoader(MNIST(fashion=True), batch_size=BS, shuffle=False, batch_tfms=[mnist_batch_tfm])\n",
    "mnist_test = DataLoader(MNIST(train=False, fashion=True), batch_size=BS, shuffle=False, batch_tfms=[mnist_batch_tfm])\n",
    "\n",
    "INT_DIM = 128\n",
    "\n",
    "w1 = Tensor(np.random.randn(784, INT_DIM) * 1, \"w1\")\n",
    "b1 = Tensor(np.ones((1, INT_DIM)) * 0.1, \"b1\")\n",
    "w2 = Tensor(np.random.randn(INT_DIM, 10) * 1, \"w2\")\n",
    "# b2 = Tensor(np.zeros((1, 10)), \"b2\")\n",
    "\n",
    "def linear_model(inputs, params, debug=list()):\n",
    "    inputs.data = inputs.data.reshape(inputs.data.shape[0], -1)\n",
    "    x = inputs\n",
    "    w1, b1, w2 = params\n",
    "    z1 = sigmoid(x.mmul(w1, \"z1\") + b1)\n",
    "    z2 = z1.mmul(w2, \"z2\")\n",
    "\n",
    "    return z2\n",
    "\n",
    "MM_func = partial(linear_model, params=[w1, b1, w2])\n",
    "optimizer = Adam([w1, b1, w2], lr=0.005)\n",
    "\n",
    "loss_f = lambda preds, targets: CrossEntropy_loss(preds, one_hot_encode_batch(targets.data, n_classes=10))\n",
    "# loss_f = lambda preds, targets: CrossEntropy_loss(preds, one_hot_encode_batch(targets.data, 10))\n",
    "\n",
    "student = Learner(\n",
    "    dataloaders=DataLoaders(mnist_train, mnist_test),\n",
    "    model=MM_func,\n",
    "    loss_func=loss_f,\n",
    "    optimizer=optimizer,\n",
    "    callbacks=[DictLoggerCallback(), ProgressBarCallback(metrics=[\"loss\", \"accuracy\",])],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68711143be744c38da7a836f52b86d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:  38%|###7      | 3/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de87a7ceae984439967fdd3cb25ccdb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep  | loss       accuracy   | val_loss   val_accuracy\n",
      "3   | 0.024697   0.906250   | 0.045255   0.849159    \n",
      "4   | 0.021336   0.906250   | 0.043722   0.851562    \n",
      "5   | 0.028579   0.843750   | 0.044103   0.851162    \n",
      "6   | 0.024253   0.875000   | 0.043021   0.851963    \n",
      "7   | 0.019325   0.968750   | 0.042728   0.853566    \n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "student.fit(epochs=5, start_epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# losses = []\n",
    "# errors = []\n",
    "# val_losses = []\n",
    "# val_errors = []\n",
    "\n",
    "# for idx, data in enumerate(student.metrics[50:]):\n",
    "#     if 'loss' in data: losses.append((idx, data['loss']))\n",
    "#     if 'accuracy' in data: errors.append((idx, 1-data['accuracy']))\n",
    "#     if 'val_loss' in data: val_losses.append((idx, data['val_loss']))\n",
    "#     if 'val_accuracy' in data: val_errors.append((idx, 1-data['val_accuracy']))\n",
    "\n",
    "# def denoise(L):\n",
    "#     i = [i for i, _ in L]\n",
    "#     x = np.array([x for _, x in L])\n",
    "\n",
    "#     window_size = 10\n",
    "#     pad_size = window_size // 2\n",
    "\n",
    "#     # Pad the array with constant values at both ends\n",
    "#     x_padded = np.pad(x, (pad_size, pad_size), 'edge')\n",
    "\n",
    "#     # Create a window for moving average\n",
    "#     window = np.ones(window_size) / window_size\n",
    "\n",
    "#     # Apply convolution on the padded array\n",
    "#     xx = np.convolve(x_padded, window, mode='valid')\n",
    "\n",
    "#     return [(p, q) for p, q in zip(i, xx)]\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# line1, = plt.plot(*zip(*losses), alpha=.3, label='Training Loss')\n",
    "# plt.plot(*zip(*denoise(losses)), color=line1.get_color(), label='Training Loss')\n",
    "# plt.plot(*zip(*val_losses), label='Validation Loss')\n",
    "# plt.title('Loss during training and validation')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# line1, = plt.plot(*zip(*errors), alpha=.3, label='Training Error')\n",
    "# plt.plot(*zip(*denoise(errors)), color=line1.get_color(), label='Training Error')\n",
    "# plt.plot(*zip(*val_errors), label='Validation Error')\n",
    "# plt.title('Error during training and validation')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
