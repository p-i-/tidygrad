# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_training.ipynb.

# %% auto 0
__all__ = ['add_callbacks', 'DictLoggerCallback', 'Learner', 'one_hot_encode_batch', 'metrics_names_pretty',
           'metrics_last_pretty', 'print_metrics_header', 'print_metrics', 'ProgressBarCallback']

# %% ../nbs/06_training.ipynb 2
from .tensor import Tensor
from .utils import noop
import numpy as np

# %% ../nbs/06_training.ipynb 3
def add_callbacks(func):
    # print("Adding callbacks", func.__name__)

    def decorator(self):
        full_name = func.__name__.replace("do_", "")
        pre_name = f"pre_{full_name}"
        post_name = f"post_{full_name}"
        for callback in self.callbacks:
            getattr(callback, pre_name, noop)(self)

        func(self)
        for callback in self.callbacks:
            getattr(callback, post_name, noop)(self)

    return decorator

# %% ../nbs/06_training.ipynb 5
class DictLoggerCallback:
    val_loss = 0
    val_error = 0

    def __init__(self, metrics=None, history=None):
        self.metrics = [MultiClassAccuracy(), Loss()] if metrics is None else metrics
        self.history = [] if history is None else history

    def log(self, learner, metric, value, accum=False, step: int = None):
        if not hasattr(learner, "history"):
            learner.history = self.history
        if step is None:
            step = self.history[-1]["step"] + 1 if self.history else 0

        if not self.history or step != self.history[-1]["step"]:
            self.history.append({"step": step})

        if metric in self.history[-1] and accum:
            self.history[-1][metric] += value
        else:
            self.history[-1][metric] = value

    def post_calc_loss(self, learner):
        for m in self.metrics:
            if learner.training:
                if m.train:
                    self.log(
                        learner,
                        f"{m.name}",
                        m.calc(learner),
                        accum=False,
                        step=learner.step,
                    )
            else:
                if m.valid:
                    self.log(
                        learner,
                        f"val_{m.name}",
                        m.calc(learner),
                        accum=True,
                        step=learner.step,
                    )

    def post_epoch(self, learner):
        for m in self.metrics:
            if f"val_{m.name}" in self.history[-1]:
                self.history[-1][f"val_{m.name}"] /= len(learner.dl)

# %% ../nbs/06_training.ipynb 6
class Learner:
    # dataloaders - train, test
    # model - function that outputs a tensor that can be fed into a loss function
    # loss_func - function that takes in a tensor and outputs a scalar
    # optimizer - Optimizer object
    def __init__(self, dataloaders, model, loss_func, optimizer, callbacks=[]):
        self.dataloaders = dataloaders
        self.model = model
        self.loss_func = loss_func
        self.optimizer = optimizer
        self.callbacks = callbacks if callbacks else []

        # The state of the learner. These are updated during training by various do_ functions
        self.training = False  # True if training, False if val/test.
        self.epoch = 0  # current epoch, starts with 1 when you start trainig.
        self.step = 0  # current step, increases by 1 every (training) batch
        self.dl = None  # current dataloader, could be train or test or val
        self.batch = None  # The current batch as a tuple of (x, y)
        self.preds: Tensor = None  # Output of the model

    def fit(self, epochs, start_epoch=0, start_step=None):
        self.start_epoch = start_epoch
        self.n_epochs = epochs
        self.step = self.step if start_step is None else start_step
        self.epochs = range(self.start_epoch, self.start_epoch + self.n_epochs)
        self.do_fit()

    @add_callbacks
    def do_fit(self):
        for e in self.epochs:
            self.epoch = e
            self.do_epoch()

    @add_callbacks
    def do_epoch(self):
        self.training = True
        self.dl = self.dataloaders.train
        self.do_all_batches()
        self.dl = self.dataloaders.test
        self.training = False
        self.do_all_batches()

    @add_callbacks
    def do_all_batches(self):
        for batch in self.dl:
            if self.training:
                self.step += 1
            self.batch = batch
            self.do_batch_forward()
            self.do_calc_loss()
            if self.training:
                self.do_batch_backward()

    @add_callbacks
    def do_calc_loss(self):
        _, y = self.batch
        self.loss = self.loss_func(self.preds, y)

    @add_callbacks
    def do_batch_forward(self):
        x, _ = self.batch
        self.preds = self.model(x)

    @add_callbacks
    def do_batch_backward(self):
        self.loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()

# %% ../nbs/06_training.ipynb 7
from tqdm.auto import tqdm

# %% ../nbs/06_training.ipynb 8
def one_hot_encode_batch(y, n_classes):
    batch_size = len(y)
    assert batch_size > 0
    assert n_classes > 0
    assert y.shape == (batch_size,)
    assert np.min(y) >= 0

    # Initialize a zero matrix of shape (batch_size, num_classes)
    one_hot_matrix = np.zeros((batch_size, n_classes))

    # Fill in the appropriate elements
    one_hot_matrix[np.arange(batch_size), y] = 1

    return Tensor(one_hot_matrix)

# %% ../nbs/06_training.ipynb 10
def metrics_names_pretty(metrics):
    """
    Return an str with the names of the metrics, extended to at least 8 characters
    """

    pretty = [f"{name:<10}" for name in metrics]
    return " ".join(pretty)


def metrics_last_pretty(metrics, metrics_dict):
    """
    Return an str with the last values of the metrics,
    extended to match the length of the metric names, min 10 characters
    """
    out = []

    for name in metrics:
        max_len = max(len(name), 10)
        if name in metrics_dict:
            value = metrics_dict[name]

            if isinstance(value, (int, str)):
                value = str(value)
            elif isinstance(value, float):
                value = f"{value:.6f}"
            else:
                value = " " * (max_len)

            if len(value) > max_len:
                value = value[: max_len - 2] + ".."

            out.append(value + " " * (max_len - len(value)))
        else:
            out.append(" " * max_len)

    return " ".join(out)

# %% ../nbs/06_training.ipynb 11
def print_metrics_header(metrics, mbar):
    mbar.write(
        "Ep  | "
        + metrics_names_pretty(metrics)
        + " | "
        + metrics_names_pretty([f"val_{m}" for m in metrics])
    )


def print_metrics(learner, metrics, mbar):
    train_metrics = {k: v for k, v in learner.history[-1].items() if k in metrics}
    val_metrics = {
        f"val_{m}": learner.history[-1].get(f"val_{m}", None) for m in metrics
    }
    mbar.write(
        f"{learner.epoch:<3} | "
        + metrics_last_pretty(metrics, train_metrics)
        + " | "
        + metrics_last_pretty([f"val_{m}" for m in metrics], val_metrics)
    )

# %% ../nbs/06_training.ipynb 12
class ProgressBarCallback:
    def __init__(self, metrics=["loss"], plot=True):
        self.metrics = metrics

    def pre_fit(self, learner):
        self.mbar = tqdm(
            initial=learner.start_epoch,
            total=learner.start_epoch + learner.n_epochs,
            desc="Epochs",
        )
        self.pbar = tqdm(leave=False)
        print_metrics_header(self.metrics, self.mbar)

    def post_epoch(self, learner):
        self.mbar.update(1)
        print_metrics(learner, self.metrics, self.mbar)
        self.mbar.set_description_str(f"Epoch")

    def pre_all_batches(self, learner):
        self.pbar.reset(total=len(learner.dl))
        if learner.training:
            self.pbar.set_description_str(f"{learner.epoch:<3}: Train")
        else:
            self.pbar.set_description_str(f"{learner.epoch:<3}: Valid")

    def post_calc_loss(self, learner):
        self.pbar.update(1)
        hist = learner.history[-1]
        metrics = [f"{k}={v:.4f}" for k, v in hist.items() if k in self.metrics]
        if not learner.training:
            val_metrics = {
                f"val_{m}": hist[f"val_{m}"] for m in self.metrics if f"val_{m}" in hist
            }
            metrics += [f"{k}={v:.4f}" for k, v in reversed(val_metrics.items())]

        self.pbar.set_postfix_str(" ".join(metrics), refresh=False)
