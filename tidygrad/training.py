# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_training.ipynb.

# %% auto 0
__all__ = ['add_callbacks', 'DictLoggerCallback', 'Learner', 'ProgressBarCallback', 'one_hot_encode_batch']

# %% ../nbs/06_training.ipynb 2
from .tensor import Tensor
from .utils import noop
import numpy as np
import fastprogress

# %% ../nbs/06_training.ipynb 3
def add_callbacks(func):
    # print("Adding callbacks", func.__name__)

    def decorator(self):
        full_name = func.__name__.replace("do_", "")
        pre_name = f"pre_{full_name}"
        post_name = f"post_{full_name}"
        for callback in self.callbacks:
            getattr(callback, pre_name, noop)(self)

        func(self)
        for callback in self.callbacks:
            getattr(callback, post_name, noop)(self)

    return decorator

# %% ../nbs/06_training.ipynb 4
class DictLoggerCallback:
    val_loss = 0
    val_error = 0

    def __init__(self, metrics=None):
        self.metrics = [] if metrics is None else metrics

    def log(self, learner, metric, value, accum=False, step: int = None):
        if not hasattr(learner, "metrics"):
            learner.metrics = self.metrics
        if step is None:
            step = self.metrics[-1]["step"] + 1 if self.metrics else 0

        if not self.metrics or step != self.metrics[-1]["step"]:
            self.metrics.append({"step": step})

        if metric in self.metrics[-1] and accum:
            self.metrics[-1][metric] += value
        else:
            self.metrics[-1][metric] = value

    def post_calc_loss(self, learner):
        if learner.training:
            self.log(learner, "loss", float(learner.loss.data), step=learner.step)
            self.log(
                learner,
                "error",
                1
                - float((learner.preds.data.argmax(1) == learner.batch[1].data).mean()),
                step=learner.step,
            )
        else:
            self.val_loss += float(learner.loss.data)
            self.val_error += 1 - float(
                (learner.preds.data.argmax(1) == learner.batch[1].data).mean()
            )
            # self.log("val_loss", float(learner.loss.data), accum=True, step=learner.step)

    def post_epoch(self, learner):
        # val_loss = self.metrics[-1]["val_loss"] / len(learner.dl)
        self.log(
            learner, "val_loss", self.val_loss / len(learner.dl), step=learner.step
        )
        self.log(
            learner, "val_error", self.val_error / len(learner.dl), step=learner.step
        )
        # self.log("epoch", learner.epoch, step=learner.step)
        self.val_loss = 0
        self.val_error = 0

# %% ../nbs/06_training.ipynb 5
class Learner:
    # dataloaders - train, test
    # model - function that outputs a tensor that can be fed into a loss function
    # loss_func - function that takes in a tensor and outputs a scalar
    # optimizer - Optimizer object
    def __init__(self, dataloaders, model, loss_func, optimizer, callbacks=[]):
        self.dataloaders = dataloaders
        self.model = model
        self.loss_func = loss_func
        self.optimizer = optimizer
        self.callbacks = callbacks if callbacks else []

        # The state of the learner. These are updated during training by various do_ functions
        self.training = False  # True if training, False if val/test.
        self.epoch = 0  # current epoch, starts with 1 when you start trainig.
        self.step = 0  # current step, increases by 1 every (training) batch
        self.dl = None  # current dataloader, could be train or test or val
        self.batch = None  # The current batch as a tuple of (x, y)
        self.preds: Tensor = None  # Output of the model

    def fit(self, epochs, start_epoch=0, start_step=None):
        self.start_epoch = start_epoch
        self.n_epochs = epochs
        self.step = self.step if start_step is None else start_step
        self.do_fit()

    @add_callbacks
    def do_fit(self):
        for e in range(self.start_epoch + 1, self.start_epoch + 1 + self.n_epochs):
            self.epoch = e
            self.do_epoch()

    @add_callbacks
    def do_epoch(self):
        self.training = True
        self.dl = self.dataloaders.train
        self.do_all_batches()
        self.dl = self.dataloaders.test
        self.training = False
        self.do_all_batches()

    @add_callbacks
    def do_all_batches(self):
        for batch in self.dl:
            if self.training:
                self.step += 1
            self.batch = batch
            self.do_batch_forward()
            self.do_calc_loss()
            if self.training:
                self.do_batch_backward()

    @add_callbacks
    def do_calc_loss(self):
        _, y = self.batch
        self.loss = self.loss_func(self.preds, y)

    @add_callbacks
    def do_batch_forward(self):
        x, _ = self.batch
        self.preds = self.model(x)

    @add_callbacks
    def do_batch_backward(self):
        self.loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()

# %% ../nbs/06_training.ipynb 6
from tqdm.auto import tqdm

# %% ../nbs/06_training.ipynb 7
class ProgressBarCallback:
    def __init__(self):
        pass

    def pre_all_batches(self, learner):
        if learner.training:
            self.pbar = tqdm(total=len(learner.dl), desc=f"Epoch {learner.epoch}")

    def post_calc_loss(self, learner):
        if learner.training:
            # print(learner.metrics[-1]["loss"])
            self.pbar.update(1)
            self.pbar.set_postfix_str(
                f"loss={learner.metrics[-1]['loss']:.4f}, error={learner.metrics[-1]['error']:.4f}"
            )

# %% ../nbs/06_training.ipynb 8
def one_hot_encode_batch(y_batch, n_classes):
    batch_size = len(y_batch)
    assert batch_size > 0
    assert n_classes > 0
    assert y_batch.shape == (batch_size,)
    assert np.min(y_batch) >= 0

    # Initialize a zero matrix of shape (batch_size, num_classes)
    one_hot_matrix = np.zeros((batch_size, n_classes))

    # Fill in the appropriate elements
    one_hot_matrix[np.arange(batch_size), y_batch] = 1

    return one_hot_matrix
