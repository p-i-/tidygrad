# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_training.ipynb.

# %% auto 0
__all__ = ['add_callbacks', 'Dataset', 'DataLoader', 'DataLoaders', 'DictLoggerCallback', 'Learner', 'one_hot_encode_batch']

# %% ../nbs/06_training.ipynb 3
from .tensor import Tensor
from .utils import noop
import numpy as np

# %% ../nbs/06_training.ipynb 4
def add_callbacks(func):
    print("Adding callbacks", func.__name__)

    def decorator(self):
        full_name = func.__name__.replace("do_", "")
        pre_name = f"pre_{full_name}"
        post_name = f"post_{full_name}"
        for callback in self.callbacks:
            getattr(callback, pre_name, noop)(self)

        func(self)
        for callback in self.callbacks:
            getattr(callback, post_name, noop)(self)

    return decorator

# %% ../nbs/06_training.ipynb 5
### Fastai style


class Dataset:
    def __getitem__(self, idx):
        raise NotImplementedError

    def __len__(self):
        raise NotImplementedError

    def shuffle(self):
        raise NotImplementedError

    def collate_fn(self, batch):
        raise NotImplementedError


class DataLoader:
    def __init__(self, dataset, batch_size=64, shuffle=True, batch_tfms=()):
        self.batch_tfms = batch_tfms
        self.dataset = dataset  # indexed dataset
        self.batch_size = batch_size
        self.shuffle = shuffle

    def __len__(self):
        return len(self.dataset) // self.batch_size

    def __iter__(self):
        if self.shuffle:
            self.dataset.shuffle()
        self.i = 0
        return self

    def __next__(self):
        if self.i + self.batch_size > len(self.dataset):
            raise StopIteration

        batch = [self.dataset[self.i + i] for i in range(self.batch_size)]
        self.i += self.batch_size
        collated = self.dataset.collate_fn(batch)
        for tfm in self.batch_tfms:
            collated = tfm(*collated)
        return [Tensor(t) for t in collated]


class DataLoaders:
    def __init__(self, train: DataLoader, test: DataLoader):
        self.train = train
        self.test = test

# %% ../nbs/06_training.ipynb 6
class DictLoggerCallback:
    val_loss = 0
    val_error = 0

    def __init__(self, metrics=None):
        self.metrics = [] if metrics is None else metrics

    def log(self, metric, value, accum=False, step: int = None):
        if step is None:
            step = self.metrics[-1]["step"] + 1 if self.metrics else 0

        if not self.metrics or step != self.metrics[-1]["step"]:
            self.metrics.append({"step": step})

        if metric in self.metrics[-1] and accum:
            self.metrics[-1][metric] += value
        else:
            self.metrics[-1][metric] = value

    def post_calc_loss(self, learner):
        if learner.training:
            self.log("loss", float(learner.loss.data), step=learner.step)
            self.log(
                "error",
                1
                - float((learner.preds.data.argmax(1) == learner.batch[1].data).mean()),
                step=learner.step,
            )
        else:
            self.val_loss += float(learner.loss.data)
            self.val_error += 1 - float(
                (learner.preds.data.argmax(1) == learner.batch[1].data).mean()
            )
            # self.log("val_loss", float(learner.loss.data), accum=True, step=learner.step)

    # def pre_epoch(self, learner):
    #

    def post_epoch(self, learner):
        # val_loss = self.metrics[-1]["val_loss"] / len(learner.dl)
        self.log("val_loss", self.val_loss / len(learner.dl), step=learner.step)
        self.log("val_error", self.val_error / len(learner.dl), step=learner.step)
        # self.log("epoch", learner.epoch, step=learner.step)
        self.val_loss = 0
        self.val_error = 0

# %% ../nbs/06_training.ipynb 7
class Learner:
    # dataloaders - train, test
    # model - function that outputs a tensor that can be fed into a loss function
    # loss_func - function that takes in a tensor and outputs a scalar
    # optimizer - Optimizer object
    def __init__(self, dataloaders, model, loss_func, optimizer, callbacks=[]):
        self.dataloaders = dataloaders
        self.model = model
        self.loss_func = loss_func
        self.optimizer = optimizer
        self.callbacks = callbacks if callbacks else []

        # The state of the learner. These are updated during training by various do_ functions
        self.training = False  # True if training, False if val/test.
        self.epoch = 0  # current epoch, starts with 1 when you start trainig.
        self.step = 0  # current step, increases by 1 every (training) batch
        self.dl = None  # current dataloader, could be train or test or val
        self.batch = None  # The current batch as a tuple of (x, y)
        self.preds: Tensor = None  # Output of the model

    def fit(self, epochs, start_epoch=0, start_step=None):
        self.start_epoch = start_epoch
        self.n_epochs = epochs
        self.step = self.step if start_step is None else start_step
        self.do_fit()

    @add_callbacks
    def do_fit(self):
        for e in range(self.start_epoch + 1, self.start_epoch + 1 + self.n_epochs):
            self.epoch = e
            self.do_epoch()

    @add_callbacks
    def do_epoch(self):
        self.training = True
        self.dl = self.dataloaders.train
        self.do_all_batches()
        self.dl = self.dataloaders.test
        self.training = False
        self.do_all_batches()

    @add_callbacks
    def do_all_batches(self):
        for batch in self.dl:
            if self.training:
                self.step += 1
            self.batch = batch
            self.do_batch_forward()
            self.do_calc_loss()
            if self.training:
                self.do_batch_backward()

    @add_callbacks
    def do_calc_loss(self):
        _, y = self.batch
        self.loss = self.loss_func(self.preds, y)

    @add_callbacks
    def do_batch_forward(self):
        x, _ = self.batch
        self.preds = self.model(x)

    @add_callbacks
    def do_batch_backward(self):
        self.loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()

# %% ../nbs/06_training.ipynb 8
import numpy as np


def one_hot_encode_batch(y_batch, n_classes):
    batch_size = len(y_batch)
    assert batch_size > 0
    assert n_classes > 0
    assert y_batch.shape == (batch_size,)
    assert np.min(y_batch) >= 0

    # Initialize a zero matrix of shape (batch_size, num_classes)
    one_hot_matrix = np.zeros((batch_size, n_classes))

    # Fill in the appropriate elements
    one_hot_matrix[np.arange(batch_size), y_batch] = 1

    return one_hot_matrix
