# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_training.ipynb.

# %% auto 0
__all__ = ['add_callbacks', 'DictLoggerCallback', 'Learner', 'ProgressBarCallback', 'one_hot_encode_batch']

# %% ../nbs/06_training.ipynb 2
from .tensor import Tensor
from .utils import noop
import numpy as np
import fastprogress

# %% ../nbs/06_training.ipynb 3
def add_callbacks(func):
    # print("Adding callbacks", func.__name__)

    def decorator(self):
        full_name = func.__name__.replace("do_", "")
        pre_name = f"pre_{full_name}"
        post_name = f"post_{full_name}"
        for callback in self.callbacks:
            getattr(callback, pre_name, noop)(self)

        func(self)
        for callback in self.callbacks:
            getattr(callback, post_name, noop)(self)

    return decorator

# %% ../nbs/06_training.ipynb 5
class DictLoggerCallback:
    val_loss = 0
    val_error = 0

    def __init__(self, metrics=None, history=None):
        self.metrics = [MultiClassAccuracy(), Loss()] if metrics is None else metrics
        self.history = [] if history is None else history

    def log(self, learner, metric, value, accum=False, step: int = None):
        if not hasattr(learner, "metrics"):
            learner.metrics = self.history
        if step is None:
            step = self.history[-1]["step"] + 1 if self.history else 0

        if not self.history or step != self.history[-1]["step"]:
            self.history.append({"step": step})

        if metric in self.history[-1] and accum:
            self.history[-1][metric] += value
        else:
            self.history[-1][metric] = value

    def post_calc_loss(self, learner):
        for m in self.metrics:
            if learner.training:
                if m.train:
                    self.log(
                        learner,
                        f"train_{m.name}",
                        m.calc(learner),
                        accum=False,
                        step=learner.step,
                    )
            else:
                if m.valid:
                    self.log(
                        learner,
                        f"val_{m.name}",
                        m.calc(learner),
                        accum=True,
                        step=learner.step,
                    )

    def post_epoch(self, learner):
        for m in self.metrics:
            if f"val_{m.name}" in self.history[-1]:
                self.history[-1][f"val_{m.name}"] /= len(learner.dl)

# %% ../nbs/06_training.ipynb 6
class Learner:
    # dataloaders - train, test
    # model - function that outputs a tensor that can be fed into a loss function
    # loss_func - function that takes in a tensor and outputs a scalar
    # optimizer - Optimizer object
    def __init__(self, dataloaders, model, loss_func, optimizer, callbacks=[]):
        self.dataloaders = dataloaders
        self.model = model
        self.loss_func = loss_func
        self.optimizer = optimizer
        self.callbacks = callbacks if callbacks else []

        # The state of the learner. These are updated during training by various do_ functions
        self.training = False  # True if training, False if val/test.
        self.epoch = 0  # current epoch, starts with 1 when you start trainig.
        self.step = 0  # current step, increases by 1 every (training) batch
        self.dl = None  # current dataloader, could be train or test or val
        self.batch = None  # The current batch as a tuple of (x, y)
        self.preds: Tensor = None  # Output of the model

    def fit(self, epochs, start_epoch=0, start_step=None):
        self.start_epoch = start_epoch
        self.n_epochs = epochs
        self.step = self.step if start_step is None else start_step
        self.do_fit()

    @add_callbacks
    def do_fit(self):
        for e in range(self.start_epoch + 1, self.start_epoch + 1 + self.n_epochs):
            self.epoch = e
            self.do_epoch()

    @add_callbacks
    def do_epoch(self):
        self.training = True
        self.dl = self.dataloaders.train
        self.do_all_batches()
        self.dl = self.dataloaders.test
        self.training = False
        self.do_all_batches()

    @add_callbacks
    def do_all_batches(self):
        for batch in self.dl:
            if self.training:
                self.step += 1
            self.batch = batch
            self.do_batch_forward()
            self.do_calc_loss()
            if self.training:
                self.do_batch_backward()

    @add_callbacks
    def do_calc_loss(self):
        _, y = self.batch
        self.loss = self.loss_func(self.preds, y)

    @add_callbacks
    def do_batch_forward(self):
        x, _ = self.batch
        self.preds = self.model(x)

    @add_callbacks
    def do_batch_backward(self):
        self.loss.backward()
        self.optimizer.step()
        self.optimizer.zero_grad()

# %% ../nbs/06_training.ipynb 7
from tqdm.auto import tqdm

# %% ../nbs/06_training.ipynb 8
class ProgressBarCallback:
    def __init__(self):
        pass

    def pre_all_batches(self, learner):
        if learner.training:
            self.pbar = tqdm(total=len(learner.dl), desc=f"Epoch {learner.epoch}")

    def post_calc_loss(self, learner):
        if learner.training:
            # print(learner.metrics[-1]["loss"])
            self.pbar.update(1)
            self.pbar.set_postfix_str(
                f"loss={learner.metrics[-1]['loss']:.4f}, error={learner.metrics[-1]['error']:.4f}"
            )

# %% ../nbs/06_training.ipynb 9
def one_hot_encode_batch(y, n_classes):
    batch_size = len(y)
    assert batch_size > 0
    assert n_classes > 0
    assert y.shape == (batch_size,)
    assert np.min(y) >= 0

    # Initialize a zero matrix of shape (batch_size, num_classes)
    one_hot_matrix = np.zeros((batch_size, n_classes))

    # Fill in the appropriate elements
    one_hot_matrix[np.arange(batch_size), y] = 1

    return Tensor(one_hot_matrix)
